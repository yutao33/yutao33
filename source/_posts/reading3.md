---
title: 100天阅读计划 -- Day 3
date: 2019-04-07 10:05:00
mathjax: true
tags: [Reading, Plan]
---


|start | end  |
|----  | -----|
|10:05 | 17:22|

> TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems




# Introduction

# Programming Model and Basic Concepts

A TensorFlow computation is described by a directed graph, which is composed of a set of nodes. The graph represents a dataflow computation, with extensions for allowing some kinds of nodes to maintain and update persistent state and for branching and looping control structures within the graph in a manner similar to Naiad

{% asset_img Figure1Figure2.png Figure1Figure2 %}

- Operations and Kernels
- Sessions
- Variables




# Implementation
The main components in a TensorFlow system are the client, which uses the Session interface to communicate with the master, and one or more worker processes, with each worker process responsible for arbitrating access to one or more computational devices (such as CPU cores or GPU cards) and for executing graph nodes on those devices as instructed by the master.

{% asset_img Figure3.png Figure3 %}

## Single-Device Execution

The nodes of the graph are executed in an order that respects the dependencies between nodes. In particular, we keep track of a count per node of the number of dependencies of that node that have not yet been executed. Once this count drops to zero, the node is eligible for execution and is added to a ready queue. The ready queue is processed in some unspecified order, delegating execution of the kernel for a node to the device object. When a node has finished executing, the counts of all nodes that depend on the completed node are decremented.

## Multi-Device Execution

### Node Placement
The placement algorithm first runs a simulated execution of the graph. The simulation is described below and ends up picking a device for each node in the graph using greedy heuristics. The node to device placement generated by this simulation is also used as the placement for the real execution.

### Cross-Device Communication

{% asset_img Figure4.png Figure4 %}

## Distributed Execution

- Fault Tolerance
    The main ones we rely on are (a) an error in a communication between a Send and Receive node pair, and (b) periodic health-checks from the master process to every worker process.






# Extensions

In this section we describe several more advanced features of the basic programming model that was introduced in Section 2.


## Gradient Computation

{% asset_img Figure5.png Figure5 %}

Figure 5 shows gradients for a cost computed from the example of Figure 2. Grey arrows show potential inputs to gradient functions that are not used for the particular operations shown.

## Partial Execution

{% asset_img Figure6.png Figure6 %}

Figure 6 shows an original graph on the left, and the transformed graph that results when Run is invoked with inputs==fbg and outputs==ff:0g. Since we only need to compute the output of node f, we will not execute nodes d and e, since they have no contribution to the output of f.

## Device Constraints

TensorFlow clients can control the placement of nodes on devices by providing partial constraints for a node about which devices it can execute on.

## Control Flow

we introduce a small set of primitive control flow operators into TensorFlow and generalize TensorFlow to handle cyclic dataflow graphs.

## Input Operations

input operation nodes in the graph, which are typically configured with a set of filenames and which yield a tensor containing one or more examples from the data stored in that set of files each time they are executed

## Queues

## Containers

A Container is the mechanism within TensorFlow for managing longer-lived mutable state. The backing store for a Variable lives in a container.







# Optimizations

- Common Subexpression Elimination
- Controlling Data Communication and Memory Usage
- Asynchronous Kernels
- Optimized Libraries for Kernel Implementations
- Lossy Compression





# Status and Experience




# Common Programming Idioms

- Data Parallel Training
- Model Parallel Training
- Concurrent Steps for Model Computation Pipelining



# Performance


# Tools

--------

{% iframe https://player.bilibili.com/player.html?aid=48373839&cid=84726739&page=1 640 430 %}
